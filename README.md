Hello! My name is Nantale Tracy Cynthia, and I am a computer engineering graduate from Busitema University. 

Currently, I am pursuing my passion for networking and cybersecurity as an intern at RENU (Research and Education Network for Uganda).

During my time at Busitema University, I gained a comprehensive understanding of computer engineering principles, including hardware, software, and network systems.
 
My coursework gave me a strong foundation in programming languages, computer architecture, and data structures.

Through my internship at RENU, I have the opportunity to apply my theoretical knowledge in a practical setting. 

Working alongside experienced professionals, I will develop skills in network administration, troubleshooting, and ensuring data security. 

My passion for networking and cybersecurity stems from a deep desire to contribute to creating secure and reliable communication infrastructures. 

I am a dedicated and proactive individual, always seeking opportunities to expand my knowledge and skills in the field. 

I had a session about operating systems and version control. 

OPERATING SYSTEMS

I got to learn that Windows is a proprietary operating system while the UNIX family is not proprietary. 
We also looked at how Linux has evolved. 

Some of the Linux branches are Slackware Linux, Debian, Redhat, CentOS, and Fedora.

I learned that when installing operating systems, several specifications are considered. Based on the purpose of the installation, one is guided on how to allocate resources, the suitable operating system, and the life of the operating system are all considered.

We also looked at how enterprises automate the installation of operating systems. 
 
VERSION CONTROL
Version control is a critical aspect of software development and technology management. 

It involves tracking and managing changes to files, code, and other digital assets over time. Version control systems (VCS) enable developers to collaborate, track modifications, and maintain a history of their work. 

Here are some key concepts and popular version control systems used in the tech industry:

Centralized Version Control Systems (CVCS): These systems, like Subversion (SVN), have a central server that stores the entire repository's history. Developers check out files from the central server, make changes locally, and then commit them back to the server. It allows for collaboration but can become a single point of failure.

Distributed Version Control Systems (DVCS): In DVCS, such as Git and Mercurial, each developer has a complete copy of the repository, including its entire history. 
This eliminates the reliance on a central server and enables offline work. 
Developers can commit changes locally, create branches, and easily merge their work with others.

Commits: A commit represents a saved snapshot of changes made to files in the repository. 
	Each commit is typically accompanied by a commit message that describes the changes made. 
	Commits provide a way to track progress and revert to previous states if needed.
Branching and Merging: Version control systems support branching, allowing developers to create separate lines of development. 
                       Branches help isolate new features or bug fixes, and once the changes are complete, they can be merged back into the main branch (often called the "master" or "main" branch).

Pull Requests: Pull requests, often used in Git-based systems, are requests made by developers to merge their changes from one branch into another (e.g., from a feature branch into the main branch). 
 It facilitates code review and ensures that changes are thoroughly examined before being merged.

Conflict Resolution: When multiple developers make conflicting changes to the same file or code section, a conflict occurs during merging. 
                 Version control systems provide tools to help resolve these conflicts manually, typically by highlighting the conflicting sections for developers to resolve.

Git is by far the most widely used version control system in the tech industry due to its distributed nature, speed, flexibility, and extensive tooling support. 
Many popular platforms like GitHub, GitLab, and Bitbucket provide hosting services for Git repositories and offer additional features like issue tracking, continuous integration, and code review.

VIRTUALIZATION AND CONTAINERIZATION

Virtualization is a technology that enables the creation of virtual versions or instances of computer hardware, software, networks, or storage resources. Common virtualization software are Virtualbox and VMware.

It allows multiple virtual environments or virtual machines (VMs) to run on a single physical machine, abstracting the underlying hardware and providing isolation between the virtual instances.

Virtualization Architecture:
There are mainly two types of virtualization architecture:  hosted architecture and bare metal.

Types of virtualization:

	Desktop virtualization

	Network virtualization 

	Storage virtualization 

Advantages of virtualization include:

Server Consolidation: Virtualization enables running multiple virtual servers on a single physical server, leading to better resource utilization and reducing the need for additional physical hardware. This consolidation can lead to cost savings regarding hardware, power consumption, and data center space.


Resource Optimization: Virtualization allows flexible allocation and reallocation of computing resources, such as CPU, memory, and storage, among virtual machines. It enables optimizing resource utilization based on the workload demands, improving overall efficiency.


Isolation and Security: Each virtual machine is isolated from others, providing a secure and contained environment. If one virtual machine is compromised or experiences issues, it does not affect the others. Additionally, virtualization allows for the creation of sandboxes for testing and development, minimizing the risk of impacting the production environment.


Easy Deployment and Scalability: Virtual machines can be quickly created, duplicated, and deployed across different physical servers or cloud platforms. This makes it easier to scale applications and services as demand increases, without the need for significant hardware changes or manual configurations.


High Availability and Fault Tolerance: Virtualization technologies often include features like live migration and high availability clustering. These capabilities allow virtual machines to be moved between physical hosts without downtime, ensuring continuous operation and minimizing the impact of hardware failures.


Disaster Recovery: Virtualization simplifies the process of backing up and restoring virtual machines, making disaster recovery plans more manageable. VM snapshots and replication mechanisms can be employed to create reliable backup copies, enabling faster recovery in case of system failures or data corruption.

Testing and Development: Virtualization provides a cost-effective and efficient way to create test environments or develop applications. Virtual machines can be quickly set up with different configurations and operating systems, allowing developers to test and debug software in isolated environments.

Overall, virtualization offers increased flexibility, resource optimization, cost savings, improved security, and simplified management, making it a fundamental technology in modern IT infrastructure.

Containerization

A containerization is a lightweight form of virtualization that allows applications and their dependencies to be packaged and isolated in containers. Containers provide a consistent and portable environment for running applications, regardless of the underlying infrastructure. Each container contains the application, along with all the necessary libraries, binaries, and configuration files.

Advantages of containerization include:

Portability

Efficiency and Performance

Scalability

Isolation

Rapid Deployment

Version Control

DevOps Enablement

Microservices Architecture

	Popular containerization technologies include Docker, which provides a platform for building and running containers, and Kubernetes, an orchestration tool for managing and scaling containerized applications. 

These tools, along with others in the container ecosystem, have revolutionized software deployment and are widely used in modern software development and deployment workflows.

After the session on virtualization and containerization, we were assigned to install windows server 2019 and ubuntu servers using accounts on VNC viewer. The assignment also included installing docker and pi-hole on both virtual machines.

We were also assigned to install an active directory on the Windows server.   

I had an exciting and fulfilling experience recently when I successfully installed Ubuntu Server, set up Docker, and deployed Pi-hole using Docker Compose. 

Throughout the process, I encountered various challenges and encountered errors, but each hurdle provided an invaluable learning opportunity.

Installing Ubuntu Server was my first step, and it involved familiarizing myself with the installation process, configuring essential settings, and ensuring the server was up and running smoothly. 

Once the server was ready, I ventured into the world of Docker, a powerful containerization platform.

Setting up Docker allowed me to encapsulate Pi-hole and its dependencies within a container, providing a consistent and isolated environment. 

Docker Compose, a tool for defining and managing multi-container applications, came in handy for orchestrating the Pi-hole deployment alongside other necessary components.

As I embarked on starting Pi-hole using Docker Compose, I encountered some challenges along the way. Errors emerged, prompting me to dig deeper into troubleshooting techniques and solutions. 

With each error message, I delved into forums, documentation, and online resources, grasping a deeper understanding of the intricacies involved. 

Through persistence and exploration, I was able to overcome the obstacles and successfully launch Pi-hole within the Docker environment.

The learning experience didn't end there. 

I expanded my knowledge by venturing into Windows Server installation and exploring Active Directory (AD). 

Installing Windows Server provided me with insights into the Windows ecosystem, and configuring AD enabled me to delve into user management, authentication, and network security.

By gaining hands-on experience with Windows Server and AD, I broadened my understanding of enterprise-level infrastructure and the importance of centralized user management and security practices. 

It was an excellent opportunity to explore the capabilities and features that Windows Server offers for organizations of varying sizes.

Overall, the journey of installing Ubuntu Server, setting up Docker, deploying Pi-hole, and venturing into Windows Server with Active Directory was a captivating experience. 

Despite encountering challenges and errors, the lessons learned and the knowledge gained was invaluable. This experience has not only expanded my technical proficiency but has also reinforced the idea that learning from mistakes and troubleshooting are integral parts of growth and expertise in the ever-evolving world of technology.
Thank you for taking the time to review my profile.

WEEK 3

In week 3, I learned the following;

SWITCHING
	* Layer 2 switching, its advantages, and limitations.

	*Differences between bridges and switches.

	*The Spanning Tree Protocol in brief 

	*LAN switch types.

VLANs

I also looked at VLANs, this included the benefits, the types (static and dynamic) as well as access and trunk links.

This also involved doing a practical lab on configuring VLANs.

	I also got to differentiate between the OSI and TCP/IP models. Also studied the format of IP addresses both IPv6 and IPv4.

	The different classes of IP addresses included private, public, etc.

	I also learned subnetting and am now capable of planning for an IP address.


ROUTING

	I learned that routing can be done by layer 3 switches and routers.

	*Learnt about static routing and managed to configure static routes in Cisco packet tracer.

	*Also learned about the different dynamic routing protocols ie. OSPF, IS-IS, BGP, and when to use them.
        
	*I and my team were able to plan for an ipv4 address and created subnets that could match our topology without wasting IP addresses.
	 
	*We then simulated the topology using a Cisco Packet tracer simulation tool and all devices had reachability with each other using static routes.
		
	*In the same week, we were grouped into 2 groups, five people each to configure a topology with is-is and the other with OSPF.
	
	*This was so challenging at first but we learned a lot and managed to achieve reachability for both protocols.

WIRELESS COMMUNICATION
	This is communication-based on radio waves. It can be used for Access Networks or Infrastructure Links. We mainly looked at Wi-Fi, an IEEE 802.11 set of wireless standards.
	
	The two prominent Access WI-Fi bands are 2.4GHz and 5GHz. 
	
	The 2.4 GHz band has 14 overlapping channels of 22MHz. Channels 1,6 and 11 are the only channels that do not overlap out of the 14.
	
	The 5 GHz band has 25 non-overlapping channels of 20 MHz but can be aggregated to 40 MHz and 80 MHz
	
	We noticed that before setting up a wireless network several considerations should be made, for example, the number of users, where the access point is to be deployed, whether wireless is the best option for the network medium, etc.

WEEK 4
	This was a systems week, and I learned a lot. It involved an introduction to the Linux operating system.
	
	We looked at the Linux shell (bash), the kernel, and system calls. 

	We also navigated the filesystem in Linux.
	
	In the practical bit, we learned commands like sudo su, adduser, changing user rights on a file/directory using chmod, and changing file and folder owners using chown.
	We also looked at the change password command "passwd", changing a user group, adding a user to a sudoers group, listing all users, listing all groups, how to create a file using the "touch" command, deleting a user,
	copying contents of one file to another, moving a file to another directory, using Ansible, the awk command, and many more.
	
	We also researched Webmin, which I found to be a web-based system administration tool for Unix-like servers that allows controlling many machines through a single interface or seamless login on other Wemin hosts on the same subnet/LAN.


WEEK 5: NETWORK MONITORING
In this week, we handled network monitoring tools. These help us keep track of the network and easily know when an issue arises. 
Cacti: (mainly monitors performance of a network device)
Cacti is an open-source network monitoring and graphing tool that helps IT professionals track and visualize the performance and health of network devices, servers, and other infrastructure components. Cacti allows users to create customized graphs and charts based on data collected from SNMP (Simple Network Management Protocol) queries. Its user-friendly web interface and advanced graph templating make it a popular choice for monitoring network bandwidth, latency, and other metrics, helping organizations optimize their network resources and troubleshoot issues efficiently.

Nagios (mainly monitors availability of a network device)
Nagios, also known as Nagios Core, is a widely-used open-source monitoring system that provides comprehensive monitoring and alerting capabilities for IT infrastructure. Nagios enables administrators to monitor the availability and performance of various services, applications, and systems. It supports active and passive checks, allowing it to collect real-time data from hosts and services. Nagios offers customizable notification alerts, enabling prompt response to incidents and minimizing downtime. Its extensible architecture and vast plugin ecosystem make it a versatile solution for monitoring a wide range of IT assets.

LibreNMS: (mainly monitors the health of the devices)
LibreNMS is a powerful open-source network monitoring and management platform designed for monitoring a diverse array of network devices and systems. It features an intuitive web-based interface and leverages SNMP to gather data from devices such as routers, switches, and servers. LibreNMS provides real-time insights into network performance, health, and availability, helping administrators proactively identify and address issues. With its robust API and integration capabilities, LibreNMS supports automated network discovery, alerting, visualization, and reporting, making it an effective tool for maintaining a stable and optimized network infrastructure.

Each of these tools plays a significant role in network and IT infrastructure monitoring, helping organizations ensure the smooth operation of their systems, detect anomalies, and respond promptly to potential problems.




